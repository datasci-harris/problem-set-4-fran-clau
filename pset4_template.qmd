---
title: "PS4 Spatial"
author: "Claudia Felipe and Francesca Leon"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points.

1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person Partner 1.
• Partner 1 (name and cnet ID): Francesca Leon (francescaleon)
• Partner 2 (name and cnet ID): Claudia Felipe (claudiafelipe)
3. Partner 1 will accept the ps4 and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted.
4. “This submission is our work alone and complies with the 30538 integrity policy.” Add your initials to indicate your agreement: **CF** **FL**
5. “I have uploaded the names of anyone else other than my partner and I worked with on the problem set here” (1 point)
6. Late coins used this pset: **0** Late coins left after submission: **4**

## Style Points (10 pts)

## Submission Steps (10 pts)

## Download and explore the Provider of Services (POS) file (10 pts)

1. The variables I pulled are: PRVDR_CTGRY_SBTYP_CD, PRVDR_CTGRY_CD, FAC_NAME, PRVDR_NUM, STATE_CD, PGM_TRMNTN_CD, ZIP_CD.

```{python}
import pandas as pd
import altair as alt
import time
import numpy as np
import warnings
import geopandas as gpd
import os
import matplotlib.pyplot as plt
import yaml

warnings.filterwarnings('ignore')
alt.renderers.enable('png')
alt.data_transformers.disable_max_rows()

base_directory = 'C:/Users/clfel/Documents/GitHub/problem-set-4-fran-clau'
```


2. 

\
    a. The number of hospitals reported are 7,245. This number doesn't make full sense because seems to be too high.
    
```{python}
pos2016 = pd.read_csv(os.path.join(base_directory, 'pos2016.csv'))
pos2016 = pos2016[(pos2016['PRVDR_CTGRY_CD']==1) & (pos2016['PRVDR_CTGRY_SBTYP_CD']==1)]
pos2016['year'] = 2016

pos2016['PRVDR_NUM'].nunique()

```

\
    b. Based on the American Hospital Association Statistics, the number of short term hospitals in 2018 (oldest available report) was 4,840. This number differs significantly from the number calculated using the database (7,245). This could happen because the hospitals are mistakenly categorized as short-term in the database. Another explanation could be that some of the short-term hospitals are closed but still appear in the database.


3. 

```{python}
pos2017 = pd.read_csv(os.path.join(base_directory, 'pos2017.csv'))
pos2017 = pos2017[(pos2017['PRVDR_CTGRY_CD']==1) & (pos2017['PRVDR_CTGRY_SBTYP_CD']==1)]
pos2017['year'] = 2017

pos2018 = pd.read_csv(os.path.join(base_directory, 'pos2018.csv'), encoding="ISO-8859-1")
pos2018 = pos2018[(pos2018['PRVDR_CTGRY_CD']==1) & (pos2018['PRVDR_CTGRY_SBTYP_CD']==1)]
pos2018['year'] = 2018

pos2019 = pd.read_csv(os.path.join(base_directory, 'pos2019.csv'), encoding="ISO-8859-1")
pos2019 = pos2019[(pos2019['PRVDR_CTGRY_CD']==1) & (pos2019['PRVDR_CTGRY_SBTYP_CD']==1)]
pos2019['year'] = 2019

pos = pd.concat([pos2016, pos2017, pos2018, pos2019], ignore_index=True)


alt.Chart(pos, title = 'Number of Observations per Year').mark_bar().encode(
  alt.X('year:O').title('Year'),
  alt.Y('count():Q').title('Number of Observations')
).properties(width = 500)
```

4. 

\
    a.

```{python}
alt.Chart(pos, title = 'Number of Unique Hospitals per Year').mark_bar().encode(
  alt.X('year:O').title('Year'),
  alt.Y('distinct(PRVDR_NUM):Q').title('Number of Unique Hospitals')
).properties(width = 500)
```

\
    b. The graphs are the same, with the same numbers. This means that the database contains the same number of observations and unique hospitals. This tells that the structure of the data is attached to the unique hospitals, those being the unit of observation.

```{python}

```

## Identify hospital closures in POS file (15 pts) (*)

1. The total suspected hospital closures is 174.

```{python}
# Filter active hospitals in 2016
active_2016 = pos2016[pos2016['PGM_TRMNTN_CD'] == 0][['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD']]

# Create a list to store the suspected closure year
closure_data = []

# Iterate over each hospital active in 2016 and check if it remains active in the following years
for _, row in active_2016.iterrows():
    provider_id = row['PRVDR_NUM']
    facility_name = row['FAC_NAME']
    zip_code = row['ZIP_CD']
    
    # Check each successive year
    if provider_id not in pos2017[pos2017['PGM_TRMNTN_CD'] == 0]['PRVDR_NUM'].values:
        closure_data.append([facility_name, zip_code, 2017])
    elif provider_id not in pos2018[pos2018['PGM_TRMNTN_CD'] == 0]['PRVDR_NUM'].values:
        closure_data.append([facility_name, zip_code, 2018])
    elif provider_id not in pos2019[pos2019['PGM_TRMNTN_CD'] == 0]['PRVDR_NUM'].values:
        closure_data.append([facility_name, zip_code, 2019])

# Convert the results into a DataFrame
closure_df = pd.DataFrame(closure_data, columns=['FAC_NAME', 'ZIP_CD', 'Year of Susp. Closure'])

print("Total suspected hospital closures:", closure_df.shape[0])

```
2. 

```{python}
# Sort hospitals by name and select the first 10 rows
sorted_hospitals = closure_df.sort_values(by='FAC_NAME')[['FAC_NAME', 'Year of Susp. Closure']].head(10)

# Display the results
sorted_hospitals.style.hide()
```
3. 

```{python}
# Count the total number of active hospitals by ZIP code for each year
active_by_zip_2016 = pos2016[pos2016['PGM_TRMNTN_CD'] == 0].groupby('ZIP_CD').size().to_dict()
active_by_zip_2017 = pos2017[pos2017['PGM_TRMNTN_CD'] == 0].groupby('ZIP_CD').size().to_dict()
active_by_zip_2018 = pos2018[pos2018['PGM_TRMNTN_CD'] == 0].groupby('ZIP_CD').size().to_dict()
active_by_zip_2019 = pos2019[pos2019['PGM_TRMNTN_CD'] == 0].groupby('ZIP_CD').size().to_dict()

# Create a DataFrame from the dictionaries
active_counts_df = pd.DataFrame({
    '2016': pd.Series(active_by_zip_2016),
    '2017': pd.Series(active_by_zip_2017),
    '2018': pd.Series(active_by_zip_2018),
    '2019': pd.Series(active_by_zip_2019)
})

# Fill any missing values with 0, in case a ZIP code has no active hospitals in a given year
active_counts_df = active_counts_df.fillna(0).astype(int)

# Create new columns for the difference in active hospital counts between consecutive years
active_counts_df['2017-2016'] = active_counts_df['2017'] - active_counts_df['2016']
active_counts_df['2018-2017'] = active_counts_df['2018'] - active_counts_df['2017']
active_counts_df['2019-2018'] = active_counts_df['2019'] - active_counts_df['2018']

active_counts_df = active_counts_df.reset_index()
active_counts_df = active_counts_df.rename(columns={'index': 'ZIP_CD'})

# Perform the merge on 'ZIP_CD'
merged_df = pd.merge(closure_df, active_counts_df, on='ZIP_CD', how='left')

#
merged_df['change'] = np.where(
        merged_df['Year of Susp. Closure'] == 2017, merged_df['2018-2017'],
        np.where(
            merged_df['Year of Susp. Closure'] == 2018, merged_df['2019-2018'],
            np.nan  
        )
    )

```

 a. The number of hospitals potentially merged/acquired is 8

```{python}
# Possible mergers or acquisitions
merged_or_acquisition = merged_df[(merged_df['change'] > 0) & (merged_df['change'].notna())]
n_merged = merged_or_acquisition.shape[0]
print("Number of hospitals potentially merged/acquired:", n_merged)

```

 b.

```{python}
# Remaining valid closures 
df_corrected = merged_df[(merged_df['change'] <= 0) & (merged_df['change'].notna())]
print("Remaining valid closures after excluding mergers/acquisitions:", df_corrected.shape[0])
```
  
  c. The remaining valid closures after excluding mergers/acquisitions is 166.

```{python}
# Sort the corrected DataFrame by hospital name and display the first 10 rows

sorted_corrected_closures = df_corrected.sort_values(by='FAC_NAME')[['FAC_NAME', 'ZIP_CD', 'Year of Susp. Closure']].head(10)
sorted_corrected_closures['ZIP_CD']=sorted_corrected_closures['ZIP_CD']\
    .astype(int)
sorted_corrected_closures.style.hide()
```

## Download Census zip code shapefile (10 pt) 

1. 

\
    a. The types of files are:\
    - .shp (shapefile): This file stores the geometric data representing the shape of geographic features.\
    - .shx (shape index file): This file contains an index of the geometry data in the .shp file, allowing for faster spatial queries and access.\
    - .dbf (database file): This file stores attribute data in a tabular format, where each row represents a feature, and columns contain the attributes or properties of that feature.\
    - .prj (projection file): This file contains information about the coordinate system and projection used, allowing the data to be correctly placed on a map.\
    - .xml (metadata file): This file contains metadata about the shapefile, such as the data source, description, and any relevant metadata standards used.


\
    b. The databases have different sizes:\
    - .shp (shapefile): 837.5 MB\
    - .shx (shape index file): 265 KB\
    - .dbf (database file): 6.4 MB\
    - .prj (projection file): 165 bytes\
    - .xml (metadata file): 16 KB\



2. 

```{python}
#Load shapefile
all_shp = gpd.read_file(os.path.join(base_directory, 'gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp'))
all_shp['ZCTA5'] = all_shp['ZCTA5'].astype(int)

#Filter shapefile to Texas zipcodes
texas_shp = all_shp[(all_shp['ZCTA5'] >= 75000) & (all_shp['ZCTA5'] <= 79900)]

#Merge shapefile with hospitals database 2016
hospitals_texas_2016 = pd.merge(texas_shp, pos2016, left_on=['ZCTA5'], right_on=['ZIP_CD'], how='left')

#Calculate number of hospitals per zipcode in 2016
hospitals_texas_2016 = hospitals_texas_2016.groupby('geometry')\
    .count()['PRVDR_NUM'].reset_index()

#Plot choropleth 
hospitals_texas_2016 = gpd.GeoDataFrame(hospitals_texas_2016, geometry='geometry')
hosp_texas = hospitals_texas_2016.plot(column='PRVDR_NUM', cmap='YlGnBu', legend=True)
plt.gcf().set_size_inches(8, 8)
hosp_texas.set_title("Number of Hospitals by ZIP Code", fontsize=15)
plt.show()
```


## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. The resulting GeoDataFrame has dimensions (33120, 3), where 33120 is the number of unique zip codes in the dataset and 2 is the number of columns. The 3 columns are:\
- ZCTA5: This column contains a unique zip code.\
- centroid: This is a point geometry column that represents the centroid of each zip code area, containing the latitude and longitude.\
- geometry: Represents an area enclosed by a series of connected lines that form a closed loop.

```{python}
#Get zipcode centroid and create GeoDataFrame
all_shp['centroid'] = all_shp.geometry.centroid
zips_all_centroids = gpd.GeoDataFrame(all_shp[['ZCTA5', 'centroid', 'geometry']], geometry='centroid')
```

2. The number of unique zipcodes in 'zips_texas_centroids' is 1,910 and in\ 'zips_texas_borderstates_centroids' is 4,057.

```{python}
#Filter Texas centroids
zips_texas_centroids = zips_all_centroids[(zips_all_centroids['ZCTA5'] >= 75000) & (zips_all_centroids['ZCTA5'] <= 79900)]

#Filter Texas and bordering states centroids
zips_texas_borderstates_centroids = zips_all_centroids[
    ((zips_all_centroids['ZCTA5'] >= 87000) & (zips_all_centroids['ZCTA5'] <= 88499)) |
    ((zips_all_centroids['ZCTA5'] >= 73000) & (zips_all_centroids['ZCTA5'] <= 74999)) |
    ((zips_all_centroids['ZCTA5'] >= 75000) & (zips_all_centroids['ZCTA5'] <= 79999)) |
    ((zips_all_centroids['ZCTA5'] >= 71600) & (zips_all_centroids['ZCTA5'] <= 72999)) |
    ((zips_all_centroids['ZCTA5'] >= 70000) & (zips_all_centroids['ZCTA5'] <= 71599))
]

#Calculate number of unique hospitals
zips_texas_centroids['ZCTA5'].nunique()
zips_texas_borderstates_centroids['ZCTA5'].nunique()
```


3. I did a left merge using 'zips_texas_borderstates_centroids' and 'pos2016' based on the variables 'ZCTA5' and 'ZIP_CD' respectively.

```{python}
#Merge with hospitals database for 2016
zips_withhospital_centroids = pd.merge(zips_texas_borderstates_centroids, pos2016, left_on=['ZCTA5'], right_on=['ZIP_CD'], how='left')

#Calculate number of hospitals by zipcode
zips_withhospital_centroids = zips_withhospital_centroids.groupby(['ZCTA5', 'centroid', 'geometry']).count()['PRVDR_NUM'].reset_index().rename(columns={'PRVDR_NUM': 'num_hospitals'})

#Filter zipcodes with at least 1 hospital
zips_withhospital_centroids = gpd.GeoDataFrame(zips_withhospital_centroids\
    [zips_withhospital_centroids['num_hospitals']>0], geometry='centroid')
```

4. 

\
    a.

```{python}
#Subset 10 zipcodes for Texas
zips_texas_centroids_10 = zips_texas_centroids.head(10)

#Run join to nearest zipcode with hospitals and calculate time for 10 zipcodes
st_10 = time.time()

join_nearest_10 = gpd.sjoin_nearest(zips_texas_centroids_10, zips_withhospital_centroids, how='inner', distance_col="distance")

et_10 = time.time()
print('Execution time:', et_10 - st_10, 'seconds')

#Estimate total time
print('I calculate the entire processing time to be:', (et_10 - st_10)*191, 'seconds')

```

\
    b. My estimation was significantly higher than the real excution time for the entire database.

```{python}
#Run join to nearest zipcode with hospitals and calculate time for entire dataset
st = time.time()

join_nearest = gpd.sjoin_nearest(zips_texas_centroids, zips_withhospital_centroids, how='inner', distance_col="distance")

et = time.time()
print('Entire processing time:', et - st, 'seconds')
```

\
    c. The .prj file is in degrees (units).

```{python}
#Convert distance in degrees to miles using 1 degree = 69 miles
join_nearest['distance'] = join_nearest['distance']*69
```

5. 

\
    a. The distance is in miles (units).

```{python}
#Calculate mean distance by zipcode
mean_distance_zip = join_nearest[['ZCTA5_left', 'distance', 'geometry_left']].groupby(['ZCTA5_left', 'geometry_left']).mean().reset_index()
```


\
    b. An average distance of 8.9 miles from any zip code in Texas to the nearest zip code with at least one hospital seems reasonable given the state's unique geography. Texas is a large state with both densely populated urban centers and vast rural areas, where hospitals are fewer and farther apart.

```{python}
#Calculate total mean distance
print('The mean total distance is:', mean_distance_zip['distance'].mean(), 'miles')
```

\
    c.

```{python}
#Plot mean distance by zipcode
mean_distance_zip = gpd.GeoDataFrame(mean_distance_zip, geometry='geometry_left')

mean_distance = mean_distance_zip.plot(column='distance', cmap='Greens', legend=True)
plt.gcf().set_size_inches(8, 8)
mean_distance.set_title("Mean Distance (in miles) to Nearest Hospital by ZIP Code", fontsize=15)
plt.show()
```
    
## Effects of closures on access in Texas (15 pts)

1. 

```{python}
df_corrected = df_corrected[['FAC_NAME', 'ZIP_CD', 'Year of Susp. Closure']]

# Filter for Texas
texas_closures = df_corrected[(df_corrected['ZIP_CD'] >= 75000) & (df_corrected['ZIP_CD'] <= 79900)]

# Count closures by ZIP code
texas_closures_zip = texas_closures.groupby('ZIP_CD').size().reset_index(name='closures')

# Summary table of the number of zipcodes vs. the number of closures they experienced
summary_table = texas_closures_zip['closures'].value_counts().reset_index()
summary_table.columns = ['Number of Closures', 'Number of ZIP Codes']
summary_table.style.hide()
```

2. There are 32 directly affected zipcodes in Texas.

```{python}

#Merge shapefile with hospitals database 2016
closures_texas = pd.merge(texas_shp, texas_closures_zip, left_on=['ZCTA5'], right_on=['ZIP_CD'], how='left')
closures_texas['closures'] = closures_texas['closures'].fillna(0)

#Create choropleth with closures by zipcode
closures_texas_plt = closures_texas.plot(column='closures', cmap='Reds', legend=True, edgecolor='gray', vmax=1)
plt.gcf().set_size_inches(8, 8)
closures_texas_plt.set_title("Number of Closures by ZIP Code", fontsize=15)
plt.show()
```

3. There are 532 indirectly affected zipcodes in TX

```{python}
#Filter directly affected zipcodes in TX
directly_affected_tx = closures_texas[closures_texas['closures']>0][['geometry', 'ZIP_CD']]

#Create 10-mile buffer
directly_affected_tx['buffer'] = directly_affected_tx.geometry.buffer(10/69)

#Get indirectly affected zipcodes in TX
indirectly_affected_tx = gpd.sjoin(
    texas_shp, directly_affected_tx.set_geometry('buffer'), how='inner', predicate='intersects')

#Remove directly affected zipcodes
indirectly_affected_tx = indirectly_affected_tx[~indirectly_affected_tx['ZCTA5'].\
    isin(directly_affected_tx['ZIP_CD'])]

#Find number of indirectly affected zip codes in TX
indirectly_affected_tx['ZCTA5'].nunique()
```

4. 

```{python}
#Create categories
texas_shp['category'] = 'Not Affected'
texas_shp.loc[texas_shp['ZCTA5'].isin(directly_affected_tx['ZIP_CD']), 'category'] = 'Directly Affected'
texas_shp.loc[texas_shp['ZCTA5'].isin(indirectly_affected_tx['ZCTA5']), 'category'] = 'Indirectly Affected'

#Choropleth with categories
fig, ax = plt.subplots(1, 1, figsize=(8, 8))
texas_shp.plot(column='category', cmap='coolwarm', legend=True, ax=ax, 
               legend_kwds={'title': 'Impact Category'})
ax.set_title("Impact of Hospital Closures on Texas ZIP Codes (2016-2019)", fontsize=15)
plt.show()
```

## Reflecting on the exercise (10 pts) 

1. The "first-pass" method for identifying hospital closures in the data has several limitations that could lead to inaccurate interpretations. One major issue is that this method might mistake cases where hospitals have been absorbed into larger networks for example. In such situations, the hospital may still be serving the community, but the method categorizes it incorrectly as a permanent closure, thus distorting the analysis of actual healthcare access impacts.\
\
On the other hand, the opposite could also occur: if a new hospital opens in the same zip code the year following a closure, this method might mistakenly interpret it as a “reopening” of the original hospital, when it is actually a new facility. This could underestimate the number of true closures and overestimate healthcare access continuity in certain areas.\
\
To improve the accuracy of this analysis, I would suggest:\
- Tracking location and name history: Checking for new facilities with similar names or close locations in subsequent years would help identify replacements rather than permanent closures.
- Monitoring certification number changes: Since some hospitals change CMS certification numbers after mergers or acquisitions, tracking these changes alongside names and locations would help confirm whether the hospital continues to operate under new management.
- Incorporating external data sources: Adding information from sources like the American Hospital Association or local health databases could verify whether a hospital is still operating, even under a different name or management, or if it has truly closed.
- Analyzing opening and closing dates: Carefully examining each hospital’s dates and other details, would help differentiate between new facilities and reopenings.

2. We consider that the way we are identifying zip codes affected by closures has some limitations, as it doesn’t necessarily capture all factors influencing access to hospitals in those areas.
\
While identifying zip codes directly affected by a closure and those within a 10-mile radius is helpful, this approach overlooks other key variables such as the capacity of remaining hospitals to absorb additional demand, the availability of transportation in rural or urban areas, and road infrastructure. These limitations make the analysis less accurate in assessing the true impact on access to healthcare services.
\
To improve this measure, we would suggest:
\
- Evaluating changes in average distance: Calculating the average distance from each zip code to the nearest hospital before and after closures, which would better reflect how geographical access has changed.
- Considering capacity and service types: Reviewing whether hospitals in nearby zip codes can handle the additional demand and if they offer services comparable to those of the closed hospital.
- Incorporating transportation data: Considering public transportation availability and road accessibility to better reflect real access in affected areas.
- These adjustments would provide a more comprehensive and accurate analysis of the impact of closures on zip code-level access to hospitals.